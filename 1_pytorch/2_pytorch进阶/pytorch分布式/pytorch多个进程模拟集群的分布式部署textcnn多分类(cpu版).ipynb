{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T02:29:10.010218Z",
     "start_time": "2019-01-10T02:29:08.378189Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.multiprocessing import Process\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import re\n",
    "import os\n",
    "from math import ceil\n",
    "from random import Random\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "from m import f1_for_car, BOW, BasicModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T02:29:18.101159Z",
     "start_time": "2019-01-10T02:29:18.097419Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "USE_CUDA = False # 用CPU\n",
    "EPOCH = 20           # 训练整批数据多少次\n",
    "BATCH_SIZE = 128\n",
    "LR = 0.002         # 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 构造embedding字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T02:29:53.190978Z",
     "start_time": "2019-01-10T02:29:39.440819Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.017 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "   Word Count: 100%|██████████| 53850/53850 [00:00<00:00, 361791.65it/s]\n",
      "Doc To Number: 100%|██████████| 53850/53850 [00:00<00:00, 195558.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# 以训练数据为例\n",
    "data_path_dir = 'data'\n",
    "data = pd.read_csv(os.path.join(data_path_dir,'cuishou_intent3.csv'),sep='\\t')\n",
    "data.columns = ['content','label']\n",
    "\n",
    "data_tmp = data.copy(deep=True)\n",
    "\n",
    "d_ = {}\n",
    "for key, value in enumerate(set(data_tmp.label)):\n",
    "    d_[value] = key\n",
    "data_tmp['label'] = data_tmp['label'].apply(lambda x : d_.get(x))\n",
    "\n",
    "y_all = np.array(data_tmp.label.tolist())\n",
    "# 构造embedding字典\n",
    "bow = BOW(data_tmp.content.apply(jieba.lcut).tolist(), min_count=1, maxlen=30) # 长度补齐或截断固定长度30\n",
    "\n",
    "vocab_size = len(bow.word2idx)\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('data/ft_wv.txt')\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size+1,300))\n",
    "for key, value in bow.word2idx.items():\n",
    "    if key in word2vec.vocab: # Word2Vec训练得到的的实例需要word2vec.wv.vocab\n",
    "        embedding_matrix[value] = word2vec.get_vector(key)\n",
    "    else:\n",
    "        embedding_matrix[value] = [0] * embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('save/embedding_matrix',arr=embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 将数据进行partition，后面送入不同的进程中执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = copy.deepcopy(bow.doc2num)\n",
    "y = copy.deepcopy(y_all)\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "for train_idx, val_idx in skf.split(X,y):\n",
    "    pass\n",
    "\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_val = X[val_idx]\n",
    "y_val = y[val_idx]\n",
    "\n",
    "# 数据处理成tensor\n",
    "train_label_tensor = torch.from_numpy(np.array(y_train)).long()\n",
    "train_content_tensor = torch.from_numpy(np.array(X_train)).long()\n",
    "dataset = Data.TensorDataset(train_content_tensor, train_label_tensor)\n",
    "\n",
    "# 验证集\n",
    "val_label_tensor = torch.from_numpy(np.array(y_val)).long()\n",
    "val_content_tensor = torch.from_numpy(np.array(X_val)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition函数\n",
    "class Partition(object):\n",
    "    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "class DataPartitioner(object):\n",
    "    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n",
    "\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        data_len = len(data)\n",
    "        indexes = [x for x in range(0, data_len)]\n",
    "        rng.shuffle(indexes)\n",
    "\n",
    "        for frac in sizes:\n",
    "            part_len = int(frac * data_len)\n",
    "            self.partitions.append(indexes[0:part_len])\n",
    "            indexes = indexes[part_len:]\n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "\n",
    "def partition_dataset(train_data_all):\n",
    "    \"\"\" Partitioning MNIST \"\"\"\n",
    "    dataset = train_data_all\n",
    "    size = dist.get_world_size()\n",
    "    bsz = int(128 / size)\n",
    "    partition_sizes = [1.0 / size for _ in range(size)]\n",
    "    partition = DataPartitioner(dataset, partition_sizes)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = torch.utils.data.DataLoader(\n",
    "        partition, batch_size=bsz, shuffle=True)\n",
    "    return train_set, bsz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 构建textCNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T02:30:17.617946Z",
     "start_time": "2019-01-10T02:30:17.603688Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 配置文件\n",
    "class Config(object):\n",
    "    '''\n",
    "    并不是所有的配置都生效,实际运行中只根据需求获取自己需要的参数\n",
    "    '''\n",
    "\n",
    "    loss = 'multilabelloss'\n",
    "    model='TextCNN' \n",
    "    title_dim = 100 # 标题的卷积核数\n",
    "    content_dim = 100 # 内容的卷积核数\n",
    "    num_classes = 21 # 类别\n",
    "    embedding_dim = 300 # embedding大小\n",
    "    linear_hidden_size = 1000 # 全连接层隐藏元数目\n",
    "    kmax_pooling = 2 # k\n",
    "    hidden_size = 128 # LSTM hidden size\n",
    "    num_layers=2 # LSTM layers\n",
    "    inception_dim = 256 #i nception的卷积核数\n",
    "    \n",
    "    kernel_size = 3 # 单尺度卷积核\n",
    "    kernel_sizes = [2,3,4] #多 尺度卷积核\n",
    "    vocab_size = vocab_size# num of words \n",
    "    content_seq_len = 30 # 内容长度 word为50 char为100\n",
    "    static = False\n",
    "    use_pretrained_embedding = True\n",
    "    embedding_path = 'save/embedding_matrix.npy'\n",
    "\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T02:30:18.176286Z",
     "start_time": "2019-01-10T02:30:18.063059Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embed_Layer(BasicModule):\n",
    "\n",
    "    def __init__(self, embedding_matrix=None, opt=None):\n",
    "        super(Embed_Layer, self).__init__()\n",
    "        self.encoder = nn.Embedding(opt.vocab_size+1, opt.embedding_dim)\n",
    "        if opt.use_pretrained_embedding:\n",
    "#             self.encoder.weight.data.copy_(t.from_numpy(np.load(embedding_path))) # 方法一，加载np.save的npy文件\n",
    "            self.encoder.weight.data.copy_(t.from_numpy(embedding_matrix)) # 方法二\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "kernel_sizes =  [1,2,3,4]\n",
    "class MultiCNNTextBNDeep(BasicModule): \n",
    "    def __init__(self, opt):\n",
    "        super(MultiCNNTextBNDeep, self).__init__()\n",
    "        self.model_name = 'MultiCNNTextBNDeep'\n",
    "        self.opt=opt\n",
    "        content_convs = [nn.Sequential(\n",
    "                                nn.Conv1d(in_channels = self.opt.embedding_dim,\n",
    "                                        out_channels = self.opt.content_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(self.opt.content_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "\n",
    "                                nn.Conv1d(in_channels = self.opt.content_dim,\n",
    "                                        out_channels = self.opt.content_dim,\n",
    "                                        kernel_size = kernel_size),\n",
    "                                nn.BatchNorm1d(self.opt.content_dim),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                # maxpool1d kernel_size=50的意思就是对一句话里每50个单词取maxpool\n",
    "                                nn.MaxPool1d(kernel_size = (self.opt.content_seq_len - kernel_size*2 + 2))\n",
    "                            )\n",
    "            for kernel_size in kernel_sizes]\n",
    "\n",
    "        self.content_convs = nn.ModuleList(content_convs)\n",
    "        \n",
    "    def forward(self, content):\n",
    "        if self.opt.static:\n",
    "            content.detach()\n",
    "        \n",
    "        content_out = [content_conv(content.permute(0,2,1)) for content_conv in self.content_convs]\n",
    "#         conv_out = t.cat((title_out+content_out),dim=1)\n",
    "        # t.cat是对list进行拼接，这里对维度1进行拼接\n",
    "        conv_out = t.cat(content_out,dim=1)\n",
    "        return conv_out\n",
    "    \n",
    "class Dense_Layer(BasicModule):\n",
    "    def __init__(self, opt=opt):\n",
    "        super(Dense_Layer, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(len(kernel_sizes)*self.opt.content_dim,self.opt.linear_hidden_size),\n",
    "            nn.BatchNorm1d(self.opt.linear_hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.opt.linear_hidden_size,self.opt.num_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        reshaped = x.view(x.size(0), -1)\n",
    "        softmax = self.fc((reshaped))\n",
    "        return softmax\n",
    "\n",
    "class Net_Main(BasicModule):\n",
    "    def __init__(self, opt=opt):\n",
    "        super(Net_Main, self).__init__()\n",
    "        self.embed_layer = Embed_Layer(embedding_matrix, opt)\n",
    "        self.multicnn = MultiCNNTextBNDeep(opt)\n",
    "        self.dense_layer = Dense_Layer(opt)\n",
    "    def forward(self, x):\n",
    "        content1 = self.embed_layer(x)\n",
    "        content2 = self.multicnn(content1)\n",
    "        res = self.dense_layer(content2)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 多进程跑模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(model):\n",
    "    \"\"\" Gradient averaging. \"\"\"\n",
    "    size = float(dist.get_world_size())\n",
    "    for param in model.parameters():\n",
    "        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\n",
    "        param.grad.data /= size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(rank, size):\n",
    "    \"\"\" Distributed Synchronous SGD Example \"\"\"\n",
    "    torch.manual_seed(1200)\n",
    "    train_loader, bsz = partition_dataset(dataset)\n",
    "    model = Net_Main(opt)\n",
    "    model = model\n",
    "#    model = model.cuda(rank)\n",
    "    # 指定优化函数和损失函数\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()   # the target label is not one-hotted 适用于多分类\n",
    "\n",
    "    num_batches = ceil(len(train_loader.dataset) / float(bsz))\n",
    "    it = 1\n",
    "    for epoch in range(EPOCH):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_id, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            loss = loss_func(output, target)\n",
    "            epoch_loss += loss.data.item()\n",
    "            optimizer.zero_grad()           # clear gradients for this training step\n",
    "            loss.backward()                 # backpropagation, compute gradients\n",
    "            average_gradients(model)\n",
    "            optimizer.step()                # apply gradients\n",
    "            it += 1\n",
    "\n",
    "        print('CPU ',\n",
    "                  dist.get_rank(), ', epoch ', epoch, ', ',\n",
    "                  'trian loss ', epoch_loss / num_batches)\n",
    "        val_output = model(val_content_tensor)\n",
    "        print('val acc: ', accuracy_score(val_label_tensor.cpu().data.numpy(), np.argmax(val_output.cpu().data.numpy(),axis=1)))\n",
    "        print('epoch {}....................................'.format(epoch))\n",
    "        del val_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_processes(rank, size, fn, backend='tcp'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU  0 , epoch  0 ,  trian loss  2.197248056309867\n",
      "CPU  1 , epoch  0 ,  trian loss  2.197548013766371\n",
      "val acc:  0.9660843709347705\n",
      "epoch 0....................................\n",
      "val acc:  0.9660843709347705\n",
      "epoch 0....................................\n",
      "CPU  0 , epoch  1 ,  trian loss  2.1527170604343584\n",
      "CPU  1 , epoch  1 ,  trian loss  2.154448325273191\n",
      "val acc:  0.9730533358111875\n",
      "epoch 1....................................\n",
      "val acc:  0.9730533358111875\n",
      "epoch 1....................................\n",
      "CPU  1 , epoch  2 ,  trian loss  2.144392533542848\n",
      "CPU  0 , epoch  2 ,  trian loss  2.1433684005227924\n",
      "val acc:  0.9749117264448988\n",
      "epoch 2....................................\n",
      "val acc:  0.9749117264448988\n",
      "epoch 2....................................\n",
      "CPU  1 , epoch  3 ,  trian loss  2.14008997951134\n",
      "CPU  0 , epoch  3 ,  trian loss  2.138311307578837\n",
      "val acc:  0.9822523694480579\n",
      "epoch 3....................................\n",
      "val acc:  0.9822523694480579\n",
      "epoch 3....................................\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    size = 2\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_processes, args=(rank, size, run))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
